{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4154e03",
   "metadata": {},
   "source": [
    "# **DIGS 30004 Final Project - \"New York Times Headlines Clustering\"**\n",
    "\n",
    "## By Franklin Wang \n",
    "\n",
    "### (Due on 03/12/2025)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d99025",
   "metadata": {},
   "source": [
    "## **1. Data Description**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39214c9",
   "metadata": {},
   "source": [
    "This dataset contains **New York Times headlines from 2017 to 2020. It aims to examine the major trends in news during the first Trump presidency**, along with metadata such as publication section and author. The goal of my final data visualization project is to discover underlying topics using methods such as **TF-IDF, PCA, and K-Means clustering**. The target audience of my work is researchers, media scholars, and students interested in media trends, political discourse, or digital humanities. **All available headlines from 2017–2020 were used without additional sampling or filtering beyond standard cleaning.** Ultimately, I am clustering these headlines into meaningful groups based on their **textual similarity**.\n",
    "\n",
    "Works Cited:\n",
    "- For TF-IDF (Text Visualization) from the course reading: \n",
    "</br>\n",
    "*Bird, Steven, Ewan Klein, and Edward Loper. Natural Language Processing with Python. O'Reilly Media, 2009.*\n",
    "\n",
    "- For PCA (Dimension Reduction - PCA) from the course reading: \n",
    "</br>\n",
    "*Jolliffe, Ian T. Principal Component Analysis. Springer, 2002, pp. 1-27.*\n",
    "\n",
    "- Plotly PCA visualization page from the week 6 Zoom lecture:\n",
    "</br>\n",
    "*Plotly. \"PCA Visualization.\" Plotly, https://plotly.com/python/pca-visualization/.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa71209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load NYT headlines 2017 dataset\n",
    "file_path_2017 = \"new_york_times_stories_2017.csv\"\n",
    "df_2017 = pd.read_csv(file_path_2017)\n",
    "\n",
    "# Select relevant columns and drop missing values\n",
    "df_2017 = df_2017[['headline', 'section', 'year']].dropna()\n",
    "df_2017.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d53eac",
   "metadata": {},
   "source": [
    "This table shows a snapshot of headlines from the New York Times in 2017, along with the section they were published in and the year. It includes a mix of topics—from sports and global news to arts and opinion—giving the audience a sense of the variety in news coverage during that time. Looking at this structure makes it easier to organize and explore patterns in the headlines. The same kind of analysis can also be done for the years 2018 through 2020 to see how topics and coverage may have changed over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9948ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load NYT headlines 2018 dataset\n",
    "file_path_2018 = \"new_york_times_stories_2018.csv\"\n",
    "df_2018 = pd.read_csv(file_path_2018)\n",
    "\n",
    "# Select relevant columns and drop missing values\n",
    "df_2018 = df_2018[['headline', 'section', 'year']].dropna()\n",
    "df_2018.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a122f743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load NYT headlines 2019 dataset\n",
    "file_path_2019 = \"new_york_times_stories_2019.csv\"\n",
    "df_2019 = pd.read_csv(file_path_2019)\n",
    "\n",
    "# Select relevant columns and drop missing values\n",
    "df_2019 = df_2019[['headline', 'section', 'year']].dropna()\n",
    "df_2019.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a34d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load NYT headlines 2020 dataset\n",
    "file_path_2020 = \"new_york_times_stories_2020.csv\"\n",
    "df_2020 = pd.read_csv(file_path_2020)\n",
    "\n",
    "# Select relevant columns and drop missing values\n",
    "df_2020 = df_2020[['headline', 'section', 'year']].dropna()\n",
    "df_2020.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7c450b",
   "metadata": {},
   "source": [
    "## **2. Research Question**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a887aabf",
   "metadata": {},
   "source": [
    "**How did New York Times headlines evolve from 2017 to 2020 during President Trump's first presidency?**\n",
    "- What were the dominant topics in each year?\n",
    "- Did the distribution of topics change significantly?\n",
    "- Was there a political news coverage increase over the years?\n",
    "\n",
    "**Potential challenges of sentiment analysis on headlines:**\n",
    "- While many headlines contain strong words that convey positivity or negativity, headlines could be mostly neutral, making classification tricky to handle.\n",
    "- Some headlines can be misleading or sarcastic, making it difficult for machines to interpret its \"human\" meaning.\n",
    "- Headlines are heavily dependent on their contexts; words may have different meanings based on external events."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e80a2fc",
   "metadata": {},
   "source": [
    "## **3. Methodology**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b56174",
   "metadata": {},
   "source": [
    "- **TF-IDF (Feature Extraction)**: Converts text into numerical features, measuring how important a word is in a document relative to a larger collection of documents (a corpus).\n",
    "- **PCA (Dimensionality Reduction)**: Reduces dimensionality for visualization and efficiently clustering simplified, complex data by keeping the most important patterns.\n",
    "- **K-Means Clustering**: Sorts data into groups based on topic similarities, organizing items into categories to identify patterns.\n",
    "\n",
    "Rationale for technique choice: TF-IDF was selected for its simplicity and interpretability in textual data, while PCA was used for visualization due to its efficiency in reducing high-dimensional data. K-Means was chosen as a straightforward and widely-used clustering algorithm suitable for numeric input from PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0876bc",
   "metadata": {},
   "source": [
    "## **4. TF-IDF Conversion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52b52e8",
   "metadata": {},
   "source": [
    "Raw text is not useful for data analysis. It needs to be converted into numbers first. That is where TF-IDF comes in.\n",
    "\n",
    "First, I clean the text, removing special characters and making everything lowercase for consistency. Then, I apply TF-IDF, which scores words based on importance—common words like “the” get ignored, while unique terms stand out.\n",
    "\n",
    "The result will be a numerical matrix where each row is a headline and each column represents a key word. This organized data can now be used for PCA to reduce dimensions and K-Means for clustering. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f336a94e",
   "metadata": {},
   "source": [
    "*P.S.: The \".apply()\" used in the following code blocks is credited to ChatGPT experimentation; it applies the cleaning function to each headline. But lambda and regular expression are learned from Dr. Gladstone and Dr. Prosser's courses.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c734bd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfidfVectorizer converts text into numerical features.\n",
    "# re (Regular Expressions) is used for text cleaning.\n",
    "# numpy is used for numerical computations.\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Preprocess 2017 text (lowercase and remove special characters)\n",
    "df_2017['cleaned_headline'] = df_2017['headline'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', x.lower())) \n",
    "\n",
    "# Convert headlines to TF-IDF representation\n",
    "# Stop words are common words like \"the\", \"and\", \"is\" that need to be removed for analysis\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "tfidf_matrix_2017 = vectorizer.fit_transform(df_2017['cleaned_headline'])\n",
    "\n",
    "# Display TF-IDF matrix shape\n",
    "# Returns (number of headlines, numbers of unique words in vocabulary)\n",
    "tfidf_matrix_2017.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8cae81",
   "metadata": {},
   "source": [
    "Finding: There were NYT 60,424 headlines in 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a48ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess 2018 text (lowercase and remove special characters)\n",
    "df_2018['cleaned_headline'] = df_2018['headline'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', x.lower()))\n",
    "\n",
    "# Convert headlines to TF-IDF representation\n",
    "# Stop words removes common words like \"the\", \"and\", \"is\"\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "tfidf_matrix_2018 = vectorizer.fit_transform(df_2018['cleaned_headline'])\n",
    "\n",
    "# Display TF-IDF matrix shape\n",
    "# Returns (number of headlines, numbers of unique words in vocabulary)\n",
    "tfidf_matrix_2018.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a93e08",
   "metadata": {},
   "source": [
    "Finding: There were NYT 58,846 headlines in 2018, seeing a decrease from last year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c23c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess 2019 text (lowercase and remove special characters)\n",
    "df_2019['cleaned_headline'] = df_2019['headline'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', x.lower()))\n",
    "\n",
    "# Convert headlines to TF-IDF representation\n",
    "# Stop words removes common words like \"the\", \"and\", \"is\"\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "tfidf_matrix_2019 = vectorizer.fit_transform(df_2019['cleaned_headline'])\n",
    "\n",
    "# Display TF-IDF matrix shape\n",
    "# Returns (number of headlines, numbers of unique words in vocabulary)\n",
    "tfidf_matrix_2019.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2c023d",
   "metadata": {},
   "source": [
    "Finding: There were NYT 53,247 headlines in 2019, seeing a continued decrease of news coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3bc5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess 2020 text (lowercase and remove special characters)\n",
    "df_2020['cleaned_headline'] = df_2020['headline'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', x.lower()))\n",
    "\n",
    "# Convert headlines to TF-IDF representation\n",
    "# Stop words removes common words like \"the\", \"and\", \"is\"\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "tfidf_matrix_2020 = vectorizer.fit_transform(df_2020['cleaned_headline'])\n",
    "\n",
    "# Display TF-IDF matrix shape\n",
    "# Returns (number of headlines, numbers of unique words in vocabulary)\n",
    "tfidf_matrix_2020.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0449ea79",
   "metadata": {},
   "source": [
    "Finding: There were NYT 55,489 headlines in 2020, seeing an increase for the first time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8104d8",
   "metadata": {},
   "source": [
    "## **5. Word Frequency Bar Chart: Finding the Most Common Words After TF-IDF**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eedbdd",
   "metadata": {},
   "source": [
    "Word frequency bar charts after TF-IDF conversion helps display the most influential words in New York Times (NYT) headlines, highlighting key topics that dominated the news. \n",
    "\n",
    "Unlike basic word counts, TF-IDF filters out common words/stop words like \"the\" or \"is\" and instead emphasizes words that are more unique and meaningful within the dataset. This allows us to see which terms—such as \"Trump,\" \"election,\" \"pandemic,\" or \"climate\"—were most significant in shaping the news narrative each year. \n",
    "\n",
    "In general, this type of visualization provides a quick, interpretable snapshot of the main themes across different time periods, making it useful for further analysis like clustering, sentiment tracking, or trend forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a1b18b",
   "metadata": {},
   "source": [
    "*P.S.: The following \".get_feature_names_out()\", \".flattern()\", and \".sort_values()\" take direct credit from ChatGPT.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4727391c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Get feature names (words) from the TF-IDF vectorizer\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Sum TF-IDF scores for each word across all headlines\n",
    "word_tfidf_sums_2017 = np.array(tfidf_matrix_2017.sum(axis=0)).flatten()\n",
    "\n",
    "# Create a dataframe of words and their corresponding TF-IDF scores\n",
    "word_tfidf_df_2017 = pd.DataFrame({'word': feature_names, 'tfidf': word_tfidf_sums_2017})\n",
    "\n",
    "# Sort words by TF-IDF score in descending order and select the top 20\n",
    "top_words_2017 = word_tfidf_df_2017.sort_values(by='tfidf', ascending=False).head(20)\n",
    "\n",
    "# Plot bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='tfidf', y='word', data=top_words_2017, palette='viridis')\n",
    "plt.xlabel(\"Total TF-IDF Score\")\n",
    "plt.ylabel(\"Word\")\n",
    "plt.title(\"Top 20 Most Common Words of NYT Headlines in 2017\")\n",
    "\n",
    "# Add source text\n",
    "plt.figtext(\n",
    "    0.5, -0.01,  # Position: centered horizontally (0.5), below the plot (-0.01)\n",
    "    \"Source: Three Decades of New York Times Headlines by Jack Bundy (Kaggle)\", \n",
    "    fontsize=10, color=\"gray\", ha='center'\n",
    ")\n",
    "\n",
    "# Show the chart\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a573fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names (words) from the TF-IDF vectorizer\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Sum TF-IDF scores for each word across all headlines\n",
    "word_tfidf_sums_2018 = np.array(tfidf_matrix_2018.sum(axis=0)).flatten()\n",
    "\n",
    "# Create a dataframe of words and their corresponding TF-IDF scores\n",
    "word_tfidf_df_2018 = pd.DataFrame({'word': feature_names, 'tfidf': word_tfidf_sums_2018})\n",
    "\n",
    "# Sort words by TF-IDF score in descending order and select the top 20\n",
    "top_words_2018 = word_tfidf_df_2018.sort_values(by='tfidf', ascending=False).head(20)\n",
    "\n",
    "# Plot bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='tfidf', y='word', data=top_words_2018, palette='viridis')\n",
    "plt.xlabel(\"Total TF-IDF Score\")\n",
    "plt.ylabel(\"Word\")\n",
    "plt.title(\"Top 20 Most Common Words of NYT Headlines in 2018\")\n",
    "\n",
    "# Add source text\n",
    "plt.figtext(\n",
    "    0.5, -0.05,  # Position: centered horizontally (0.5), below the plot (-0.01)\n",
    "    \"Source: Three Decades of New York Times Headlines by Jack Bundy (Kaggle)\", \n",
    "    fontsize=10, color=\"gray\", ha='center'\n",
    ")\n",
    "\n",
    "# Show the chart\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e49a856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names (words) from the TF-IDF vectorizer\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Sum TF-IDF scores for each word across all headlines\n",
    "word_tfidf_sums_2019 = np.array(tfidf_matrix_2019.sum(axis=0)).flatten()\n",
    "\n",
    "# Create a dataframe of words and their corresponding TF-IDF scores\n",
    "word_tfidf_df_2019 = pd.DataFrame({'word': feature_names, 'tfidf': word_tfidf_sums_2019})\n",
    "\n",
    "# Sort words by TF-IDF score in descending order and select the top 20\n",
    "top_words_2019 = word_tfidf_df_2019.sort_values(by='tfidf', ascending=False).head(20)\n",
    "\n",
    "# Plot bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='tfidf', y='word', data=top_words_2019, palette='viridis')\n",
    "plt.xlabel(\"Total TF-IDF Score\")\n",
    "plt.ylabel(\"Word\")\n",
    "plt.title(\"Top 20 Most Common Words of NYT Headlines in 2019\")\n",
    "\n",
    "# Add source text\n",
    "plt.figtext(\n",
    "    0.5, -0.05,  # Position: centered horizontally (0.5), below the plot (-0.01)\n",
    "    \"Source: Three Decades of New York Times Headlines by Jack Bundy (Kaggle)\", \n",
    "    fontsize=10, color=\"gray\", ha='center'\n",
    ")\n",
    "\n",
    "# Show the chart\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22022a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names (words) from the TF-IDF vectorizer\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Sum TF-IDF scores for each word across all headlines\n",
    "word_tfidf_sums_2020 = np.array(tfidf_matrix_2020.sum(axis=0)).flatten()\n",
    "\n",
    "# Create a dataframe of words and their corresponding TF-IDF scores\n",
    "word_tfidf_df_2020 = pd.DataFrame({'word': feature_names, 'tfidf': word_tfidf_sums_2020})\n",
    "\n",
    "# Sort words by TF-IDF score in descending order and select the top 20\n",
    "top_words_2020 = word_tfidf_df_2020.sort_values(by='tfidf', ascending=False).head(20)\n",
    "\n",
    "# Plot bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='tfidf', y='word', data=top_words_2020, palette='viridis')\n",
    "plt.xlabel(\"Total TF-IDF Score\")\n",
    "plt.ylabel(\"Word\")\n",
    "plt.title(\"Top 20 Most Common Words of NYT Headlines in 2020\")\n",
    "\n",
    "# Add source text\n",
    "plt.figtext(\n",
    "    0.5, -0.05,  # Position: centered horizontally (0.5), below the plot (-0.01)\n",
    "    \"Source: Three Decades of New York Times Headlines by Jack Bundy (Kaggle)\", \n",
    "    fontsize=10, color=\"gray\", ha='center'\n",
    ")\n",
    "\n",
    "# Show the chart\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7665e6f",
   "metadata": {},
   "source": [
    "## **6. Dimensionality Reduction with PCA**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94180b41",
   "metadata": {},
   "source": [
    "The following arrays array([x, y]) represent the explained variance ratios of the first two principal components (PC1 and PC2) in Principal Component Analysis (PCA).\n",
    "\n",
    "We cannot definitively interpret what PCA Component 1 and Component 2 represent because PCA does not label features—it only finds the directions of maximum variance in the dataset. However, we can observe patterns in how different years' headlines cluster and infer what topics or trends might be influencing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa7a204",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Reduce dimensionality with PCA 2017\n",
    "pca = PCA(n_components=2)\n",
    "tfidf_pca_2017 = pca.fit_transform(tfidf_matrix_2017.toarray())\n",
    "\n",
    "# Display explained variance ratio\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57c92f1",
   "metadata": {},
   "source": [
    "- PC1 explains 1.42% of the total variance in the data.\n",
    "- PC2 explains 1.11% of the total variance in the data.\n",
    "- Together, PC1 + PC2 explain only 2.53% of the total variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355f5595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scatter plot of PCA 2017 results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(tfidf_pca_2017[:, 0], tfidf_pca_2017[:, 1], marker='o')\n",
    "\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "plt.title(\"PCA of NYT Headlines (2017)\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.figtext(\n",
    "    0.5, -0.05,  \n",
    "    \"Source: Three Decades of New York Times Headlines by Jack Bundy (Kaggle)\", \n",
    "    fontsize=10, color=\"gray\", ha='center'\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df46e4e8",
   "metadata": {},
   "source": [
    "This plot shows that a lot of the headlines from 2017 were quite similar, clustering closely together. That likely means the New York Times was covering a few major topics repeatedly throughout the year. At the same time, the scatter of some points suggests there were also more unique or varied headlines that stood out from the main themes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2460e608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensionality with PCA 2018\n",
    "pca = PCA(n_components=2)\n",
    "tfidf_pca_2018 = pca.fit_transform(tfidf_matrix_2018.toarray())\n",
    "\n",
    "# Display explained variance ratio\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa73534d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of PCA 2018 results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(tfidf_pca_2018[:, 0], tfidf_pca_2018[:, 1], marker='o')\n",
    "\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "plt.title(\"PCA of NYT Headlines (2018)\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.figtext(\n",
    "    0.5, -0.05,  \n",
    "    \"Source: Three Decades of New York Times Headlines by Jack Bundy (Kaggle)\", \n",
    "    fontsize=10, color=\"gray\", ha='center'\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5286979d",
   "metadata": {},
   "source": [
    "This PCA plot of 2018 NYT headlines shows a similar pattern to 2017, with a dense cluster near the center—indicating that many headlines covered overlapping or repeated topics. However, the points are slightly more spread out compared to 2017, which may suggest that the headlines in 2018 covered a slightly wider range of themes or events, leading to a bit more variety in the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a094c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensionality with PCA 2019\n",
    "pca = PCA(n_components=2)\n",
    "tfidf_pca_2019 = pca.fit_transform(tfidf_matrix_2019.toarray())\n",
    "\n",
    "# Display explained variance ratio\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438135e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scatter plot of PCA results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(tfidf_pca_2019[:, 0], tfidf_pca_2019[:, 1], marker='o')\n",
    "\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "plt.title(\"PCA of NYT Headlines (2019)\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.figtext(\n",
    "    0.5, -0.05,  \n",
    "    \"Source: Three Decades of New York Times Headlines by Jack Bundy (Kaggle)\", \n",
    "    fontsize=10, color=\"gray\", ha='center'\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2c38bf",
   "metadata": {},
   "source": [
    "This PCA plot of 2019 NYT headlines shows a wider spread than earlier years, with points beginning to branch out in different directions. That means there may have been more variety in the topics covered or the language used in headlines that year. There is still a dense cluster in the center, showing that many headlines shared similar themes—but the extended patterns suggest a shift toward broader or more distinct coverage areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c108a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensionality with PCA\n",
    "pca = PCA(n_components=2)\n",
    "tfidf_pca_2020 = pca.fit_transform(tfidf_matrix_2020.toarray())\n",
    "\n",
    "# Display explained variance ratio\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4557a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scatter plot of PCA results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(tfidf_pca_2020[:, 0], tfidf_pca_2020[:, 1], marker='o')\n",
    "\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "plt.title(\"PCA of NYT Headlines (2020)\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.figtext(\n",
    "    0.5, -0.05,  \n",
    "    \"Source: Three Decades of New York Times Headlines by Jack Bundy (Kaggle)\", \n",
    "    fontsize=10, color=\"gray\", ha='center'\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5581469f",
   "metadata": {},
   "source": [
    "This PCA plot of 2020 New York Times headlines shows a dense, stretched-out pattern along the horizontal axis, which tells us that most headlines were closely focused around a few major themes. That makes sense—2020 was marked by huge global events like the COVID-19 pandemic and the U.S. presidential election. Because of that, the language in headlines was more repetitive, with similar words and topics showing up again and again. So while there were still differences between headlines, many were pulled into the same direction by the overwhelming focus on these historic moments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017b1f76",
   "metadata": {},
   "source": [
    "## **7. Plotly Express**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f60035d",
   "metadata": {},
   "source": [
    "Visualizing All the Original Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dbe73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Define file paths\n",
    "file_paths = {\n",
    "    2017: \"new_york_times_stories_2017.csv\",\n",
    "    2018: \"new_york_times_stories_2018.csv\",\n",
    "    2019: \"new_york_times_stories_2019.csv\",\n",
    "    2020: \"new_york_times_stories_2020.csv\"\n",
    "}\n",
    "\n",
    "# Load and clean data\n",
    "df_all_years = pd.DataFrame()\n",
    "for year, path in file_paths.items():\n",
    "    df = pd.read_csv(path, usecols=['headline']).dropna()\n",
    "    df['year'] = year\n",
    "    df['cleaned_headline'] = df['headline'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', str(x).lower()))\n",
    "    df_all_years = pd.concat([df_all_years, df], ignore_index=True)\n",
    "\n",
    "# TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "tfidf_matrix = vectorizer.fit_transform(df_all_years['cleaned_headline'])\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=4)\n",
    "pca_result = pca.fit_transform(tfidf_matrix.toarray())\n",
    "\n",
    "# Create PCA DataFrame\n",
    "df_pca = pd.DataFrame(pca_result, columns=['PC1', 'PC2', 'PC3', 'PC4'])\n",
    "df_pca['year'] = df_all_years['year'].values\n",
    "\n",
    "# Reorder to draw 2020 first, 2017 last\n",
    "year_order = [2020, 2019, 2018, 2017]\n",
    "df_pca = df_pca[df_pca['year'].isin(year_order)]\n",
    "df_pca['year'] = pd.Categorical(df_pca['year'], categories=year_order, ordered=True)\n",
    "df_pca = df_pca.sort_values('year', ascending=False)\n",
    "\n",
    "# Plot scatter matrix\n",
    "fig = px.scatter_matrix(\n",
    "    df_pca,\n",
    "    dimensions=['PC1', 'PC2', 'PC3', 'PC4'],\n",
    "    color=df_pca['year'].astype(str),  # Keep original Plotly color mapping\n",
    "    title=\"Pairwise PCA Component Scatter Plots (NYT Headlines 2017–2020)\",\n",
    "    labels={'year': 'Year'}\n",
    ")\n",
    "\n",
    "fig.update_traces(diagonal_visible=False)\n",
    "\n",
    "# Add data source annotation\n",
    "fig.add_annotation(\n",
    "    text=\"Source: Three Decades of New York Times Headlines by Jack Bundy (Kaggle)\",\n",
    "    xref=\"paper\", yref=\"paper\",\n",
    "    x=0.5, y=-0.3,\n",
    "    showarrow=False,\n",
    "    font=dict(size=12, color=\"gray\"),\n",
    "    xanchor='center'\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f28eac",
   "metadata": {},
   "source": [
    "Visualizing All the Principal Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a3416b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Define file paths for datasets (update paths as needed)\n",
    "file_paths = {\n",
    "    2017: \"new_york_times_stories_2017.csv\",\n",
    "    2018: \"new_york_times_stories_2018.csv\",\n",
    "    2019: \"new_york_times_stories_2019.csv\",\n",
    "    2020: \"new_york_times_stories_2020.csv\"\n",
    "}\n",
    "\n",
    "# Create an empty DataFrame to store all years\n",
    "df_all_years = pd.DataFrame()\n",
    "\n",
    "# Process each year’s dataset\n",
    "for year, path in file_paths.items():\n",
    "    df = pd.read_csv(path, usecols=['headline']).dropna()\n",
    "    df['year'] = year\n",
    "    df['cleaned_headline'] = df['headline'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', str(x).lower()))\n",
    "    df_all_years = pd.concat([df_all_years, df], ignore_index=True)\n",
    "\n",
    "# Convert headlines to a TF-IDF feature matrix\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "tfidf_matrix = vectorizer.fit_transform(df_all_years['cleaned_headline'])\n",
    "\n",
    "# Apply PCA to reduce dimensions to 4\n",
    "pca = PCA(n_components=4)\n",
    "components = pca.fit_transform(tfidf_matrix.toarray())\n",
    "\n",
    "# Create labels with explained variance percentages\n",
    "labels = {\n",
    "    str(i): f\"PC {i+1} ({var:.1f}%)\"\n",
    "    for i, var in enumerate(pca.explained_variance_ratio_ * 100)\n",
    "}\n",
    "\n",
    "# Create an interactive PCA scatter matrix plot\n",
    "fig = px.scatter_matrix(\n",
    "    pd.DataFrame(components, columns=[f\"PC{i+1}\" for i in range(4)]).assign(Year=df_all_years['year']),\n",
    "    labels=labels,\n",
    "    dimensions=[f\"PC{i+1}\" for i in range(4)],\n",
    "    color=df_all_years['year'].astype(str),\n",
    "    title=\"Pairwise PCA Component Scatter Plots (NYT Headlines 2017-2020)\"\n",
    ")\n",
    "\n",
    "# Hide diagonal histograms for better visualization\n",
    "fig.update_traces(diagonal_visible=False)\n",
    "\n",
    "# Add data source\n",
    "fig.add_annotation(\n",
    "    text=\"Source: Three Decades of New York Times Headlines by Jack Bundy (Kaggle)\",\n",
    "    xref=\"paper\", yref=\"paper\",\n",
    "    x=0.5, y=-0.3,\n",
    "    showarrow=False,\n",
    "    font=dict(size=12, color=\"gray\"),\n",
    "    xanchor='center'\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74754ddb",
   "metadata": {},
   "source": [
    "Visualizing a Subset of the Principal Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d678d1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Define file paths for datasets\n",
    "file_paths = {\n",
    "    2017: \"new_york_times_stories_2017.csv\",\n",
    "    2018: \"new_york_times_stories_2018.csv\",\n",
    "    2019: \"new_york_times_stories_2019.csv\",\n",
    "    2020: \"new_york_times_stories_2020.csv\"\n",
    "}\n",
    "\n",
    "# Create an empty DataFrame to store all years\n",
    "df_all_years = pd.DataFrame()\n",
    "\n",
    "# Process each year’s dataset\n",
    "for year, path in file_paths.items():\n",
    "    df = pd.read_csv(path, usecols=['headline']).dropna()\n",
    "    df['year'] = year\n",
    "    df['cleaned_headline'] = df['headline'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', str(x).lower()))\n",
    "    df_all_years = pd.concat([df_all_years, df], ignore_index=True)\n",
    "\n",
    "# Convert headlines to a TF-IDF feature matrix\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "tfidf_matrix = vectorizer.fit_transform(df_all_years['cleaned_headline'])\n",
    "\n",
    "# Apply PCA to reduce dimensions to 2\n",
    "n_components = 2\n",
    "pca = PCA(n_components=n_components)\n",
    "components = pca.fit_transform(tfidf_matrix.toarray())\n",
    "\n",
    "# Compute total explained variance\n",
    "total_var = pca.explained_variance_ratio_.sum() * 100\n",
    "\n",
    "# Create labels for PCA components\n",
    "labels = {str(i): f\"PC {i+1}\" for i in range(n_components)}\n",
    "labels['color'] = 'Year'\n",
    "\n",
    "# Create an interactive PCA scatter matrix plot\n",
    "fig = px.scatter_matrix(\n",
    "    pd.DataFrame(components, columns=[f\"PC{i+1}\" for i in range(n_components)]).assign(Year=df_all_years['year']),\n",
    "    color='Year',\n",
    "    dimensions=[f\"PC{i+1}\" for i in range(n_components)],\n",
    "    labels=labels,\n",
    "    title=f'Total Explained Variance: {total_var:.2f}% (NYT Headlines 2017-2020)'\n",
    ")\n",
    "\n",
    "# Hide diagonal histograms for better readability\n",
    "fig.update_traces(diagonal_visible=False)\n",
    "\n",
    "# Add data source\n",
    "fig.add_annotation(\n",
    "    text=\"Source: Three Decades of New York Times Headlines by Jack Bundy (Kaggle)\",\n",
    "    xref=\"paper\", yref=\"paper\",\n",
    "    x=0.5, y=-0.3,\n",
    "    showarrow=False,\n",
    "    font=dict(size=12, color=\"gray\"),\n",
    "    xanchor='center'\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418e5caa",
   "metadata": {},
   "source": [
    "A 2.32% explained variance from PCA on TF-IDF data is **not wrong**. In fact, ChatGPT suggests that it is completely **expected** in natural language processing. TF-IDF creates a very high-dimensional, sparse matrix where most words are rare and features are weakly correlated. This means there is no dominant direction of variation for PCA to capture, so the variance gets spread thin across many components. As a result, even the first two principal components might only account for a small percentage of the total variance. **This does not make my visualization bad**—it can still reveal meaningful structure like topic clusters or yearly shifts. However, if I am aiming to compress or explain more variance, ChatGPT suggests that it can improved using methods like Truncated SVD (which is better suited for sparse data) or dense embeddings like BERT, which capture semantic meaning more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1889b155",
   "metadata": {},
   "source": [
    "## **8. 2D PCA Scatter Plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c16892",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize TF-IDF matrix before PCA\n",
    "tfidf_matrix_array = tfidf_matrix.toarray()\n",
    "tfidf_matrix_scaled = StandardScaler().fit_transform(tfidf_matrix_array)\n",
    "\n",
    "# Apply PCA on standardized data\n",
    "pca = PCA(n_components=2)\n",
    "pca_components = pca.fit_transform(tfidf_matrix_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090472c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_sample_size = df_all_years['year'].value_counts().min()\n",
    "\n",
    "df_balanced = df_all_years.groupby('year').apply(lambda x: x.sample(min_sample_size, random_state=42)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670ac1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load datasets for 2017, 2018, 2019, and 2020 (Update file paths)\n",
    "file_paths = {\n",
    "    2017: \"new_york_times_stories_2017.csv\",\n",
    "    2018: \"new_york_times_stories_2018.csv\",\n",
    "    2019: \"new_york_times_stories_2019.csv\",\n",
    "    2020: \"new_york_times_stories_2020.csv\"\n",
    "}\n",
    "\n",
    "# Create an empty DataFrame to store all years\n",
    "df_all_years = pd.DataFrame()\n",
    "\n",
    "# Process each year’s dataset\n",
    "for year, path in file_paths.items():\n",
    "    df = pd.read_csv(path, usecols=['headline']).dropna()\n",
    "    df['year'] = year\n",
    "    df['cleaned_headline'] = df['headline'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', x.lower()))\n",
    "    df_all_years = pd.concat([df_all_years, df], ignore_index=True)\n",
    "\n",
    "# Convert headlines to TF-IDF representation\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "tfidf_matrix = vectorizer.fit_transform(df_all_years['cleaned_headline'])\n",
    "\n",
    "# Perform PCA to reduce dimensions to 2\n",
    "pca = PCA(n_components=2)\n",
    "pca_components = pca.fit_transform(tfidf_matrix.toarray())\n",
    "\n",
    "# Add PCA components to DataFrame\n",
    "df_pca = pd.DataFrame(pca_components, columns=['PC1', 'PC2'])\n",
    "df_pca['year'] = df_all_years['year'].values\n",
    "\n",
    "# Create an interactive 2D PCA scatter plot for all years\n",
    "fig = px.scatter(\n",
    "    df_pca, x='PC1', y='PC2', \n",
    "    color=df_pca['year'].astype(str),\n",
    "    opacity = 0.7,\n",
    "    title=\"2D PCA Scatterplot of NYT Headlines (2017-2020)\",\n",
    "    labels={'PC1': 'PCA Component 1', 'PC2': 'PCA Component 2', 'year': 'Year'}\n",
    ")\n",
    "\n",
    "# Add data source\n",
    "fig.add_annotation(\n",
    "    text=\"Source: Three Decades of New York Times Headlines by Jack Bundy (Kaggle)\",\n",
    "    xref=\"paper\", yref=\"paper\",\n",
    "    x=0.5, y=-0.3,\n",
    "    showarrow=False,\n",
    "    font=dict(size=12, color=\"gray\"),\n",
    "    xanchor='center'\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n",
    "\n",
    "# Save the interactive plot as an HTML\n",
    "fig.write_html(\"2-D PCA.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfcb889",
   "metadata": {},
   "source": [
    "This PCA scatterplot shows how NYT headlines from 2017 to 2020 are grouped based on patterns in their word usage. Each dot represents a headline, and similar headlines are positioned closer together. While there are some distinct clusters, most points from all four years tend to overlap, suggesting that the general themes and language used in the headlines remained fairly consistent over time. The purple dots from 2020 appear more dominant—not necessarily because the headlines were drastically different, but simply due to how many there were and the order in which they were plotted. This visualization gives a broad look at how headline topics evolved—or didn’t—across those four years."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93207981",
   "metadata": {},
   "source": [
    "## **9. Interactive Dash App for PCA on NYT Headlines**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4bb0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dash import Dash, dcc, html, Input, Output\n",
    "import pandas as pd\n",
    "import re\n",
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load and process NYT datasets\n",
    "file_paths = {\n",
    "    2017: \"new_york_times_stories_2017.csv\",\n",
    "    2018: \"new_york_times_stories_2018.csv\",\n",
    "    2019: \"new_york_times_stories_2019.csv\",\n",
    "    2020: \"new_york_times_stories_2020.csv\"\n",
    "}\n",
    "\n",
    "df_all_years = pd.DataFrame()\n",
    "for year, path in file_paths.items():\n",
    "    df = pd.read_csv(path, usecols=['headline']).dropna()\n",
    "    df['year'] = year\n",
    "    df['cleaned_headline'] = df['headline'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', str(x).lower()))\n",
    "    df_all_years = pd.concat([df_all_years, df], ignore_index=True)\n",
    "\n",
    "# Convert headlines to a TF-IDF feature matrix\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "tfidf_matrix = vectorizer.fit_transform(df_all_years['cleaned_headline'])\n",
    "\n",
    "# Initialize Dash app\n",
    "app = Dash(__name__)\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.H4(\"Visualization of PCA's Explained Variance on NYT Headlines (2017-2020)\"),\n",
    "    dcc.Graph(id=\"graph\"),\n",
    "    html.P(\"Number of PCA Components:\"),\n",
    "    dcc.Slider(\n",
    "        id='slider',\n",
    "        min=2, max=6, value=3, step=1, \n",
    "        marks={i: str(i) for i in range(2, 7)}\n",
    "    )\n",
    "])\n",
    "\n",
    "@app.callback(\n",
    "    Output(\"graph\", \"figure\"), \n",
    "    Input(\"slider\", \"value\"))\n",
    "def run_and_plot(n_components):\n",
    "    # Perform PCA with selected components\n",
    "    pca = PCA(n_components=n_components)\n",
    "    components = pca.fit_transform(tfidf_matrix.toarray())\n",
    "    \n",
    "    # Compute total explained variance\n",
    "    var = pca.explained_variance_ratio_.sum() * 100\n",
    "\n",
    "    # Define labels\n",
    "    labels = {str(i): f\"PC {i+1}\" for i in range(n_components)}\n",
    "    labels['color'] = 'Year'\n",
    "\n",
    "    # Create interactive scatter matrix plot\n",
    "    fig = px.scatter_matrix(\n",
    "        pd.DataFrame(components, columns=[f\"PC{i+1}\" for i in range(n_components)]).assign(Year=df_all_years['year']),\n",
    "        color='Year',\n",
    "        dimensions=[f\"PC{i+1}\" for i in range(n_components)],\n",
    "        labels=labels,\n",
    "        title=f'Total Explained Variance: {var:.2f}% (NYT Headlines 2017-2020)'\n",
    "    )\n",
    "\n",
    "    fig.update_traces(diagonal_visible=False)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Run the Dash server\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a475658",
   "metadata": {},
   "source": [
    "The increase from 2.3% to 3.21% in explained variance simply means that more principal components were used in the PCA analysis, allowing slightly more of the underlying structure of the TF-IDF matrix to be captured. This is a normal and expected outcome when dealing with high-dimensional, sparse text data like TF-IDF, where each dimension (word) contributes only a small amount of variation. Even though 3.21% may seem low, ChatGPT suggests that it is typical in natural language processing and still useful for visualizing clusters or patterns in reduced dimensions. The low variance does not imply poor results—it reflects the nature of text data and the linear limitations of PCA. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155597e6",
   "metadata": {},
   "source": [
    "## 10. **Visualize PCA with px.scatter_3d**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcf3d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Define file paths for datasets (update paths as needed)\n",
    "file_paths = {\n",
    "    2017: \"new_york_times_stories_2017.csv\",\n",
    "    2018: \"new_york_times_stories_2018.csv\",\n",
    "    2019: \"new_york_times_stories_2019.csv\",\n",
    "    2020: \"new_york_times_stories_2020.csv\"\n",
    "}\n",
    "\n",
    "# Create an empty DataFrame to store all years\n",
    "df_all_years = pd.DataFrame()\n",
    "\n",
    "# Process each year’s dataset\n",
    "for year, path in file_paths.items():\n",
    "    df = pd.read_csv(path, usecols=['headline']).dropna()\n",
    "    df['year'] = year\n",
    "    df['cleaned_headline'] = df['headline'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', str(x).lower()))\n",
    "    df_all_years = pd.concat([df_all_years, df], ignore_index=True)\n",
    "\n",
    "# Apply PCA to reduce dimensions to 3 for 3D visualization\n",
    "pca = PCA(n_components=3)\n",
    "components = pca.fit_transform(tfidf_matrix.toarray())\n",
    "\n",
    "# Compute total explained variance\n",
    "total_var = pca.explained_variance_ratio_.sum() * 100\n",
    "\n",
    "# Create an interactive 3D scatter plot\n",
    "fig = px.scatter_3d(\n",
    "    pd.DataFrame(components, columns=['PC1', 'PC2', 'PC3']).assign(Year=df_all_years['year']),\n",
    "    x='PC1', y='PC2', z='PC3',\n",
    "    color=df_all_years['year'].astype(str),\n",
    "    title=f'Total Explained Variance: {total_var:.2f}% (NYT Headlines 2017-2020)',\n",
    "    labels={'PC1': 'PC 1', 'PC2': 'PC 2', 'PC3': 'PC 3'}\n",
    ")\n",
    "\n",
    "# Add data source\n",
    "fig.add_annotation(\n",
    "    text=\"Source: Three Decades of New York Times Headlines by Jack Bundy (Kaggle)\",\n",
    "    xref=\"paper\", yref=\"paper\",\n",
    "    x=0.5, y=-0.3,\n",
    "    showarrow=False,\n",
    "    font=dict(size=12, color=\"gray\"),\n",
    "    xanchor='center'\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64c8aca",
   "metadata": {},
   "source": [
    "This 3D scatter plot shows how New York Times headlines from 2017 to 2020 are distributed after being transformed into a reduced-dimensional space using PCA. Each dot represents a headline, positioned based on three principal components (PC1, PC2, PC3), and colored by year. The fact that the points overlap heavily and cluster near the center reflects that the differences in headline content across years are subtle when reduced to three dimensions. The total explained variance of only 3.21% indicates that the first three principal components capture just a small portion of the overall information in the original high-dimensional TF-IDF data, which is expected with text data. While the visualization does not clearly separate years, it still helps illustrate the overall textual similarity across this four-year period."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca26e1da",
   "metadata": {},
   "source": [
    "## **10. Clustering with K-Means**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2567de",
   "metadata": {},
   "source": [
    "This section is aided completely by ChatGPT for the purpose of experimentation. ChatGPT helps streamline complex explanations of K-Means and provides optimized code that follows best practices in PCA + clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60c7c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Elbow Method to find the best K\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Define file paths for datasets (update paths as needed)\n",
    "file_paths = {\n",
    "    2017: \"new_york_times_stories_2017.csv\",\n",
    "    2018: \"new_york_times_stories_2018.csv\",\n",
    "    2019: \"new_york_times_stories_2019.csv\",\n",
    "    2020: \"new_york_times_stories_2020.csv\"\n",
    "}\n",
    "\n",
    "# Create an empty DataFrame to store all years\n",
    "df_all_years = pd.DataFrame()\n",
    "\n",
    "# Process each year’s dataset\n",
    "for year, path in file_paths.items():\n",
    "    df = pd.read_csv(path, usecols=['headline']).dropna()\n",
    "    df['year'] = year\n",
    "    df['cleaned_headline'] = df['headline'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', str(x).lower()))\n",
    "    df_all_years = pd.concat([df_all_years, df], ignore_index=True)\n",
    "\n",
    "# Convert headlines to a TF-IDF feature matrix\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "tfidf_matrix = vectorizer.fit_transform(df_all_years['cleaned_headline'])\n",
    "\n",
    "# Apply PCA to reduce dimensions to 3 for clustering\n",
    "pca = PCA(n_components=3)\n",
    "pca_result = pca.fit_transform(tfidf_matrix.toarray())\n",
    "\n",
    "# Use the Elbow Method to determine optimal K\n",
    "inertia = []\n",
    "k_values = range(2, 10)\n",
    "\n",
    "for k in k_values:\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    km.fit(pca_result)\n",
    "    inertia.append(km.inertia_)\n",
    "\n",
    "# Plot the Elbow Curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(k_values, inertia, marker='o', linestyle='--')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Inertia (Sum of Squared Distances)')\n",
    "plt.title('Elbow Method for Optimal K (NYT Headlines 2017-2020)')\n",
    "\n",
    "plt.figtext(\n",
    "    0.5, -0.05,  \n",
    "    \"Source: Three Decades of New York Times Headlines by Jack Bundy (Kaggle)\", \n",
    "    fontsize=10, color=\"gray\", ha='center'\n",
    ")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761e95e3",
   "metadata": {},
   "source": [
    "ChatGPT explanation credit: \n",
    "- The result of this plot represents the Elbow Method, a common technique used to determine the optimal number of clusters (K) for K-Means clustering. In this case, it is applied to the PCA-reduced TF-IDF representation of New York Times headlines from 2017 to 2020. The y-axis shows the inertia, which measures how tightly grouped the data points are within each cluster — lower values indicate better fit. As K increases, inertia naturally decreases, but the rate of improvement slows after a certain point. The “elbow” of the curve (a noticeable bend) suggests the optimal number of clusters where adding more clusters yields diminishing returns. Identifying that elbow allows us to choose a K that balances performance with simplicity. For NYT headlines, this helps discover the number of meaningful topic groupings that naturally emerge in the data without overfitting.\n",
    "\n",
    "Finding: \n",
    "- The sharp drop in inertia between K=2 and K=4 suggests that adding more clusters significantly improves the model up to that point. After K=4, the curve begins to flatten, indicating diminishing returns. This \"elbow\" at K=4 suggests that 4 clusters is a reasonable choice for capturing the main patterns in the headlines without overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c4c10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-Means clustering\n",
    "num_clusters = 5\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
    "df_all_years['cluster'] = kmeans.fit_predict(pca_result)\n",
    "\n",
    "# Display sample headlines with assigned clusters\n",
    "df_all_years[['headline', 'cluster']].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01df230",
   "metadata": {},
   "source": [
    "ChatGPT explanation credit:\n",
    "- In K-Means clustering, cluster 0 does not have an inherent meaning. It is simply one of the group labels assigned by the algorithm. For the NYT headlines data, cluster 0 represents a group of headlines that share similar language patterns or topics based on their TF-IDF features. To interpret what cluster 0 actually means, exploring the most common words or themes within that group is needed, such as by viewing sample headlines or generating a word cloud.\n",
    "\n",
    "What does it mean?\n",
    "- For example, headlines assigned to cluster 0 might share a general theme (e.g., sports, obituaries, or daily summaries), while cluster 3 might capture political or technology-related stories. This clustering step is useful for uncovering hidden patterns or dominant topics in the headlines without needing to pre-label or categorize the data manually. It is especially valuable for exploring large-scale news datasets to understand how themes evolve over time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880621cd",
   "metadata": {},
   "source": [
    "## **11. Cluster Visualization in PCA Space**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7629bc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Convert headlines to a TF-IDF feature matrix\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "tfidf_matrix = vectorizer.fit_transform(df_all_years['cleaned_headline'])\n",
    "\n",
    "# Apply PCA to reduce dimensions to 2 for visualization\n",
    "pca = PCA(n_components=2)\n",
    "tfidf_pca = pca.fit_transform(tfidf_matrix.toarray())\n",
    "\n",
    "# Apply K-Means Clustering with the optimal K (e.g., 5 clusters)\n",
    "optimal_k = 5  # This should be determined using the Elbow Method\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "df_all_years['cluster'] = kmeans.fit_predict(tfidf_pca)\n",
    "\n",
    "# Scatter plot of clusters in PCA-reduced space\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(\n",
    "    x=tfidf_pca[:, 0], \n",
    "    y=tfidf_pca[:, 1], \n",
    "    hue=df_all_years['cluster'], \n",
    "    palette='viridis', \n",
    "    alpha=0.7\n",
    ")\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.title('K-Means Clustering of NYT Headlines (PCA-reduced space)')\n",
    "plt.legend(title='Cluster')\n",
    "\n",
    "plt.figtext(\n",
    "    0.5, -0.05,  \n",
    "    \"Source: Three Decades of New York Times Headlines by Jack Bundy (Kaggle)\", \n",
    "    fontsize=10, color=\"gray\", ha='center'\n",
    ")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62925e51",
   "metadata": {},
   "source": [
    "## **12. Topic Clustering with K-Means**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221396d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df_2017, df_2018, df_2019, df_2020], ignore_index=True)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Preprocess text (lowercase and remove special characters)\n",
    "df_all['cleaned_headline'] = df_all['headline'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', x.lower()))\n",
    "\n",
    "# Convert headlines to TF-IDF representation\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "tfidf_matrix = vectorizer.fit_transform(df_all['cleaned_headline'])\n",
    "\n",
    "# Display TF-IDF matrix shape\n",
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9217ea95",
   "metadata": {},
   "source": [
    "Finding: The shape (228006, 1000) means that the dataset contains 228,006 headlines, each represented by a vector of 1,000 TF-IDF scores corresponding to the most important words across all headlines from 2017 to 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92990d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Reduce dimensionality with PCA\n",
    "pca = PCA(n_components=2)\n",
    "tfidf_pca = pca.fit_transform(tfidf_matrix.toarray())\n",
    "\n",
    "# Apply K-Means clustering\n",
    "num_clusters = 5\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
    "df_all['cluster'] = kmeans.fit_predict(tfidf_pca)\n",
    "\n",
    "# Display sample headlines with assigned clusters\n",
    "df_all[['headline', 'year', 'cluster']].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef6f4a3",
   "metadata": {},
   "source": [
    "## **13. Topic Distribution Over Time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dff212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter for only valid years\n",
    "df_all = df_all[df_all['year'].isin([2017, 2018, 2019, 2020])]\n",
    "\n",
    "# Count plot of clusters by year\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x=df_all['cluster'], hue=df_all['year'], palette='viridis')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Number of Headlines')\n",
    "plt.title('Distribution of NYT Headline Topics from 2017 to 2020')\n",
    "plt.legend(title='Year')\n",
    "\n",
    "plt.figtext(\n",
    "    0.5, -0.05,  \n",
    "    \"Source: Three Decades of New York Times Headlines by Jack Bundy (Kaggle)\", \n",
    "    fontsize=10, color=\"gray\", ha='center'\n",
    ")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec4cd6f",
   "metadata": {},
   "source": [
    "This bar chart breaks down how NYT headlines from 2017 to 2020 were grouped into different clusters based on topic similarity. Cluster 0 dominates by a large margin each year, which means most headlines likely fell into a broad or general category. The other clusters had significantly fewer headlines, suggesting they represent more specific or less frequently covered topics. Interestingly, the shape of the distribution looks pretty consistent across the years, which could mean that the main types of news stories did not change much over time—even if the details did."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faef9d7",
   "metadata": {},
   "source": [
    "## **14. Word Cloud per Cluster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209da366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit: All code below is an experiment of ChatGPT.\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Copy to keep original intact\n",
    "df_all = df_all_years.copy()\n",
    "\n",
    "# Fill any missing cleaned headlines\n",
    "df_all['cleaned_headline'] = df_all['cleaned_headline'].fillna('')\n",
    "\n",
    "# Make sure 'cluster' column exists (if not already assigned)\n",
    "# Assumes pca_result has been computed earlier and matches df_all shape\n",
    "num_clusters = 5\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
    "df_all['cluster'] = kmeans.fit_predict(pca_result)\n",
    "\n",
    "# ✅ Generate Word Cloud for Each Cluster (0–4)\n",
    "for cluster in range(num_clusters):\n",
    "    cluster_data = df_all[df_all['cluster'] == cluster]\n",
    "\n",
    "    if cluster_data.empty:\n",
    "        print(f\"No data for cluster {cluster}, skipping...\")\n",
    "        continue\n",
    "\n",
    "    text = \" \".join(cluster_data['cleaned_headline'].dropna().tolist())\n",
    "\n",
    "    if not text.strip():\n",
    "        print(f\"No valid text for cluster {cluster}, skipping word cloud...\")\n",
    "        continue\n",
    "\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Word Cloud for Cluster {cluster}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58edbf4f",
   "metadata": {},
   "source": [
    "## **15. Word Cloud per Year**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7495c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit: All code below is an experiment of ChatGPT.\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Rename df_all_years for clarity (if needed)\n",
    "df_all = df_all_years.copy()\n",
    "\n",
    "# Ensure cleaned_headline exists and is filled\n",
    "df_all['cleaned_headline'] = df_all['cleaned_headline'].fillna('')\n",
    "\n",
    "# Generate Word Cloud per year\n",
    "for year in [2017, 2018, 2019, 2020]:\n",
    "    # Filter only rows from the year\n",
    "    year_data = df_all[df_all['year'] == year]\n",
    "\n",
    "    if year_data.empty:\n",
    "        print(f\"No headlines for {year}, skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Join cleaned headlines into a single string\n",
    "    text = \" \".join(year_data['cleaned_headline'].dropna().tolist())\n",
    "\n",
    "    # Skip empty text\n",
    "    if not text.strip():\n",
    "        print(f\"No valid text for {year}, skipping word cloud...\")\n",
    "        continue\n",
    "\n",
    "    # Create the word cloud\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "\n",
    "    # Plot the word cloud\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Word Cloud for {year}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b583a650",
   "metadata": {},
   "source": [
    "## **16. Outlook & Future Improvements**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77454284",
   "metadata": {},
   "source": [
    "### Limitations of the Current Approach\n",
    "\n",
    "My project explores New York Times headlines by turning them into numbers using TF-IDF and then visualizing patterns through PCA and clustering. TF-IDF helps us see which words are important in each headline, but it does not understand the meaning behind them. So, two headlines about the same event but using different words might end up looking completely unrelated in the data.\n",
    "\n",
    "PCA simplifies the data so we can plot it, but it only keeps a tiny slice of the full picture—just 2 to 3 percent of the information. That means the charts give us a rough sketch, not a detailed map. K-Means clustering groups headlines into categories, but it assumes that topics are nicely separated and evenly shaped, which is not how real language works. Plus, headlines are short and often vague, so it is tough to capture what they are really about just from the words they use.\n",
    "\n",
    "### Potential Improvements and Future Directions\n",
    "\n",
    "Hypothetically, there are a few ways I could make this analysis more meaningful if I have mastered more advanced data visualization skills. Instead of using TF-IDF, which just looks at word frequency, ChatGPT suggests that I could switch to more advanced models like BERT. These models actually understand the context of words, so headlines with similar meanings would be grouped together—even if they use different vocabulary. For visualization, methods like t-SNE or UMAP might do a better job than PCA at showing how headlines relate to each other in a clearer, more natural way.\n",
    "\n",
    "Realistically, I could improve things by including more than just the headline text given that there is more time allocated to the planning of my project. For example, knowing what section the article came from, when it was published, or who wrote it could help us track how topics evolved over time or varied between sections. And instead of basic clustering, I could use topic modeling tools to uncover deeper themes in collections of text. This could lead to more insightful, easier-to-understand topic groups that better reflect how readers actually make sense of the news.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intro_to_programming",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
